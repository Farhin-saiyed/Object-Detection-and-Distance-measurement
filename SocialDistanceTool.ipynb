{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc_Lj0mcKGJJ"
   },
   "source": [
    "### Download the code zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8155,
     "status": "ok",
     "timestamp": 1624203938222,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "SfVsGRegJcq7",
    "outputId": "e76066d3-46f8-499a-9b41-15d8c8582533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-20 15:45:29--  https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-social-distancing-detector/social-distance-detector.zip\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.245.152\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.245.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 266848888 (254M) [application/zip]\n",
      "Saving to: ‘social-distance-detector.zip’\n",
      "\n",
      "social-distance-det 100%[===================>] 254.49M  60.3MB/s    in 4.4s    \n",
      "\n",
      "2021-06-20 15:45:34 (58.5 MB/s) - ‘social-distance-detector.zip’ saved [266848888/266848888]\n",
      "\n",
      "/content/social-distance-detector\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/opencv-social-distancing-detector/social-distance-detector.zip\n",
    "!unzip -qq social-distance-detector.zip\n",
    "%cd social-distance-detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQl-4t1OKQ3l"
   },
   "source": [
    "## Blog Post Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU6IIjjeKUdb"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 947,
     "status": "ok",
     "timestamp": 1624203948716,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "t9g021RoKQNW"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAakXkQL5ate"
   },
   "source": [
    "### Function to display images in Jupyter Notebooks and Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1624203952384,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "2R7wEnD85YRd"
   },
   "outputs": [],
   "source": [
    "def plt_imshow(title, image):\n",
    "    # convert the image frame BGR to RGB color space and display it\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\tplt.imshow(image)\n",
    "\tplt.title(title)\n",
    "\tplt.grid(False)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p01dOCe7a73"
   },
   "source": [
    "### Our configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1624203960046,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "7T5pqddg7coT"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # base path to YOLO directory\n",
    "    MODEL_PATH = \"yolo-coco\"\n",
    "\n",
    "    # initialize minimum probability to filter weak detections along with\n",
    "    # the threshold when applying non-maxima suppression\n",
    "    MIN_CONF = 0.3\n",
    "    NMS_THRESH = 0.3\n",
    "\n",
    "    # boolean indicating if NVIDIA CUDA GPU should be used\n",
    "    USE_GPU = False\n",
    "\n",
    "    # define the minimum safe distance (in pixels) that two people can be\n",
    "    # from each other\n",
    "    MIN_DISTANCE = 50\n",
    "\n",
    "# instantiate our Config object\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ5jrYvT7tIT"
   },
   "source": [
    "### Detecting people in images and video streams with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1624203963102,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "OOqROrK27s8B"
   },
   "outputs": [],
   "source": [
    "def detect_people(frame, net, ln, personIdx=0):\n",
    "\t# grab the dimensions of the frame and  initialize the list of\n",
    "\t# results\n",
    "\t(H, W) = frame.shape[:2]\n",
    "\tresults = []\n",
    "\n",
    "    # construct a blob from the input frame and then perform a forward\n",
    "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
    "\t# and associated probabilities\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tlayerOutputs = net.forward(ln)\n",
    "\n",
    "\t# initialize our lists of detected bounding boxes, centroids, and\n",
    "\t# confidences, respectively\n",
    "\tboxes = []\n",
    "\tcentroids = []\n",
    "\tconfidences = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "\tfor output in layerOutputs:\n",
    "\t\t# loop over each of the detections\n",
    "\t\tfor detection in output:\n",
    "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
    "\t\t\t# of the current object detection\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\t# filter detections by (1) ensuring that the object\n",
    "\t\t\t# detected was a person and (2) that the minimum\n",
    "\t\t\t# confidence is met\n",
    "\t\t\tif classID == personIdx and confidence > config.MIN_CONF:\n",
    "\t\t\t\t# scale the bounding box coordinates back relative to\n",
    "\t\t\t\t# the size of the image, keeping in mind that YOLO\n",
    "\t\t\t\t# actually returns the center (x, y)-coordinates of\n",
    "\t\t\t\t# the bounding box followed by the boxes' width and\n",
    "\t\t\t\t# height\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
    "\t\t\t\t# and left corner of the bounding box\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\t# update our list of bounding box coordinates,\n",
    "\t\t\t\t# centroids, and confidences\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "\t# bounding boxes\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, config.MIN_CONF, config.NMS_THRESH)\n",
    "\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "\t\t\t# update our results list to consist of the person\n",
    "\t\t\t# prediction probability, bounding box coordinates,\n",
    "\t\t\t# and the centroid\n",
    "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "\t\t\tresults.append(r)\n",
    "\n",
    "\t# return the list of results\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wegd_Pv58Jby"
   },
   "source": [
    "### Implementing a social distancing detector with OpenCV and deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1624203968611,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "6Yk-lRE3Klu3"
   },
   "outputs": [],
   "source": [
    "# construct the argument parse and parse the arguments\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
    "#\thelp=\"path to (optional) input video file\")\n",
    "#ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
    "#\thelp=\"path to (optional) output video file\")\n",
    "#ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
    "#\thelp=\"whether or not output frame should be displayed\")\n",
    "#args = vars(ap.parse_args())\n",
    "\n",
    "# since we are using Jupyter Notebooks we can replace our argument\n",
    "# parsing code with *hard coded* arguments and values\n",
    "args = {\n",
    "    \"input\": \"pedestrians.mp4\",\n",
    "    \"output\": \"output.avi\",\n",
    "    \"display\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1624203972600,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "DE_aohvx8aGE"
   },
   "outputs": [],
   "source": [
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = os.path.sep.join([config.MODEL_PATH, \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([config.MODEL_PATH, \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([config.MODEL_PATH, \"yolov3.cfg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1624203976423,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "z433nMSF8c-4",
    "outputId": "753de22f-94c3-425c-ae94-2beb0ce5c86f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n"
     ]
    }
   ],
   "source": [
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "# check if we are going to use GPU\n",
    "if config.USE_GPU:\n",
    "\t# set CUDA as the preferable backend and target\n",
    "\tprint(\"[INFO] setting preferable backend and target to CUDA...\")\n",
    "\tnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "\tnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1624203979085,
     "user": {
      "displayName": "Sanjeev Kumar",
      "photoUrl": "",
      "userId": "03884654528492753901"
     },
     "user_tz": 240
    },
    "id": "R_gX6IfI8fYe",
    "outputId": "09961ecb-4bc9-4ce5-e3d3-cdb26c5dcf4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] accessing video stream...\n"
     ]
    }
   ],
   "source": [
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# initialize the video stream and pointer to output video file\n",
    "print(\"[INFO] accessing video stream...\")\n",
    "vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-IpW8JSt8iKZ"
   },
   "outputs": [],
   "source": [
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "\t# read the next frame from the file\n",
    "\t(grabbed, frame) = vs.read()\n",
    "\n",
    "\t# if the frame was not grabbed, then we have reached the end\n",
    "\t# of the stream\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\n",
    "\t# resize the frame and then detect people (and only people) in it\n",
    "\tframe = imutils.resize(frame, width=700)\n",
    "\tresults = detect_people(frame, net, ln,\n",
    "\t\tpersonIdx=LABELS.index(\"person\"))\n",
    "\n",
    "\t# initialize the set of indexes that violate the minimum social\n",
    "\t# distance\n",
    "\tviolate = set()\n",
    "\n",
    "    # ensure there are *at least* two people detections (required in\n",
    "\t# order to compute our pairwise distance maps)\n",
    "\tif len(results) >= 2:\n",
    "\t\t# extract all centroids from the results and compute the\n",
    "\t\t# Euclidean distances between all pairs of the centroids\n",
    "\t\tcentroids = np.array([r[2] for r in results])\n",
    "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "\t\t# loop over the upper triangular of the distance matrix\n",
    "\t\tfor i in range(0, D.shape[0]):\n",
    "\t\t\tfor j in range(i + 1, D.shape[1]):\n",
    "\t\t\t\t# check to see if the distance between any two\n",
    "\t\t\t\t# centroid pairs is less than the configured number\n",
    "\t\t\t\t# of pixels\n",
    "\t\t\t\tif D[i, j] < config.MIN_DISTANCE:\n",
    "\t\t\t\t\t# update our violation set with the indexes of\n",
    "\t\t\t\t\t# the centroid pairs\n",
    "\t\t\t\t\tviolate.add(i)\n",
    "\t\t\t\t\tviolate.add(j)\n",
    "\n",
    "    # loop over the results\n",
    "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "\t\t# extract the bounding box and centroid coordinates, then\n",
    "\t\t# initialize the color of the annotation\n",
    "\t\t(startX, startY, endX, endY) = bbox\n",
    "\t\t(cX, cY) = centroid\n",
    "\t\tcolor = (0, 255, 0)\n",
    "\n",
    "\t\t# if the index pair exists within the violation set, then\n",
    "\t\t# update the color\n",
    "\t\tif i in violate:\n",
    "\t\t\tcolor = (0, 0, 255)\n",
    "\n",
    "\t\t# draw (1) a bounding box around the person and (2) the\n",
    "\t\t# centroid coordinates of the person,\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "\n",
    "\t# draw the total number of social distancing violations on the\n",
    "\t# output frame\n",
    "\ttext = \"Social Distancing Violations: {}\".format(len(violate))\n",
    "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
    "\n",
    "    # check to see if the output frame should be displayed to our\n",
    "\t# screen\n",
    "\tif args[\"display\"] > 0:\n",
    "\t\t# show the output frame\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\t\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t\t# if the `q` key was pressed, break from the loop\n",
    "\t\tif key == ord(\"q\"):\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# if an output video file path has been supplied and the video\n",
    "\t# writer has not been initialized, do so now\n",
    "\tif args[\"output\"] != \"\" and writer is None:\n",
    "\t\t# initialize our video writer\n",
    "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n",
    "\t\t\t(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "\t# if the video writer is not None, write the frame to the output\n",
    "\t# video file\n",
    "\tif writer is not None:\n",
    "\t\twriter.write(frame)\n",
    "\n",
    "# do a bit of cleanup\n",
    "vs.release()\n",
    "\n",
    "# check to see if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "\twriter.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpSXMhHhHOJw"
   },
   "source": [
    "Note that the above code block may take time to execute. If you are interested to view the video within Colab just execute the following code blocks. Note that it may be time-consuming.\n",
    "\n",
    "Our output video is produced in `.avi` format. First, we need to convert it to `.mp4` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lhYbtMwnHTp-"
   },
   "outputs": [],
   "source": [
    "!ffmpeg -i output.avi output.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "UJCPzNUbHk8n"
   },
   "outputs": [],
   "source": [
    "#@title Display video inline\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(\"output.mp4\", \"rb\").read()\n",
    "dataURL = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % dataURL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2fKnX3lHrWX"
   },
   "source": [
    "This code is referred from [this StackOverflow thread](https://stackoverflow.com/a/57378660/7636462). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHqTPsQVSry_"
   },
   "source": [
    "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*OpenCV Social Distancing Detector*](https://www.pyimagesearch.com/2020/06/01/opencv-social-distancing-detector/) blog post published on 2020-06-01."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNVQt1/MeiIVBgGlVGht8ZT",
   "name": "SocialDistanceTool.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
